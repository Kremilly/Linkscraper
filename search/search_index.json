{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Dive deep into the web's intricate layers with Linkscraper! Whether you're a researcher, developer, or a curious explorer, our tool efficiently scans web pages to fetch links, images, emails, and much more. Powered by an array of versatile plugins and a user-friendly interface, Linkscraper streamlines the process of extracting and managing web data. From headers to JavaScript files, and from cookies to CSS \u2013 uncover the digital signatures of the web with ease. Join us on this journey and uncover the treasures hidden in plain sight on the web. Requirements Python >= 3.10 ( Download ) PIP Getting Started Clone this repository. git clone https://github.com/kremilly/linkscraper.git To install dependencies. pip install -r requirements.txt","title":"Getting Started"},{"location":"#introduction","text":"Dive deep into the web's intricate layers with Linkscraper! Whether you're a researcher, developer, or a curious explorer, our tool efficiently scans web pages to fetch links, images, emails, and much more. Powered by an array of versatile plugins and a user-friendly interface, Linkscraper streamlines the process of extracting and managing web data. From headers to JavaScript files, and from cookies to CSS \u2013 uncover the digital signatures of the web with ease. Join us on this journey and uncover the treasures hidden in plain sight on the web.","title":"Introduction"},{"location":"#requirements","text":"Python >= 3.10 ( Download ) PIP","title":"Requirements"},{"location":"#getting-started","text":"Clone this repository. git clone https://github.com/kremilly/linkscraper.git To install dependencies. pip install -r requirements.txt","title":"Getting Started"},{"location":"basics/cookies/","text":"Cookies Retrieves all cookies from the site. python linkscraper -u https://example.com -a get-cookies This will return a list of all the cookies used by https://example.com . Filter The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-cookies -filter cookie To collect cookies from the website \" https://example.com \" and then filter the results to only show those related to the term \" cookie \"","title":"Get cookies"},{"location":"basics/cookies/#cookies","text":"Retrieves all cookies from the site. python linkscraper -u https://example.com -a get-cookies This will return a list of all the cookies used by https://example.com .","title":"Cookies"},{"location":"basics/cookies/#filter","text":"The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-cookies -filter cookie To collect cookies from the website \" https://example.com \" and then filter the results to only show those related to the term \" cookie \"","title":"Filter"},{"location":"basics/core/","text":"Core The core function is the first function that can be executed by the application, it provides the main functions concerning the connection to a certain URL that was provided by you. You can execute the function by running the following command: python linkscraper -u https://example.com -a get-core When you run the above command, it will fetch and request headers of the URL https://example.com .","title":"Core data"},{"location":"basics/core/#core","text":"The core function is the first function that can be executed by the application, it provides the main functions concerning the connection to a certain URL that was provided by you. You can execute the function by running the following command: python linkscraper -u https://example.com -a get-core When you run the above command, it will fetch and request headers of the URL https://example.com .","title":"Core"},{"location":"basics/get-emails/","text":"Get Emails By using various parameters, users can define specific actions, like collecting email addresses from the web pages. In the context of the provided command: python linkscraper -u https://example.com -a get-emails The flag is directed to scrape the website \" https://example.com \" with the specific action ( -a ) of retrieving email addresses ( get-emails ). This allows users to gather emails present on the given site, which can be useful for various purposes, including research, auditing, or data collection. As always, such a tool should be used ethically and with proper permissions to avoid any legal or ethical violations. Filter The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-emails -filter example@domain.com To collect emails from the website \" https://example.com \" and then filter the results to only show those related to the email \" example@doamin.com \"","title":"Get emails"},{"location":"basics/get-emails/#get-emails","text":"By using various parameters, users can define specific actions, like collecting email addresses from the web pages. In the context of the provided command: python linkscraper -u https://example.com -a get-emails The flag is directed to scrape the website \" https://example.com \" with the specific action ( -a ) of retrieving email addresses ( get-emails ). This allows users to gather emails present on the given site, which can be useful for various purposes, including research, auditing, or data collection. As always, such a tool should be used ethically and with proper permissions to avoid any legal or ethical violations.","title":"Get Emails"},{"location":"basics/get-emails/#filter","text":"The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-emails -filter example@domain.com To collect emails from the website \" https://example.com \" and then filter the results to only show those related to the email \" example@doamin.com \"","title":"Filter"},{"location":"basics/get-links/","text":"Get Links The -get-links command is designed to extract all links from a user-specified URL. Along with its primary function, it supports three additional sub-commands that we will detail below. The command allows for tailored actions through different parameters. A common use case is to harvest links from web pages, as demonstrated: python linkscraper -u https://example.com -a get-links Filter The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-links -filter domain.com Only external links The -oel option allows users to refine their results by including only those entries that match the links outside from -u . python linkscraper -u https://example.com -a get-emails -oel Show status code The -ssc option show the status code of all links listed python linkscraper -u https://example.com -a get-emails -ssc","title":"Get links"},{"location":"basics/get-links/#get-links","text":"The -get-links command is designed to extract all links from a user-specified URL. Along with its primary function, it supports three additional sub-commands that we will detail below. The command allows for tailored actions through different parameters. A common use case is to harvest links from web pages, as demonstrated: python linkscraper -u https://example.com -a get-links","title":"Get Links"},{"location":"basics/get-links/#filter","text":"The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-links -filter domain.com","title":"Filter"},{"location":"basics/get-links/#only-external-links","text":"The -oel option allows users to refine their results by including only those entries that match the links outside from -u . python linkscraper -u https://example.com -a get-emails -oel","title":"Only external links"},{"location":"basics/get-links/#show-status-code","text":"The -ssc option show the status code of all links listed python linkscraper -u https://example.com -a get-emails -ssc","title":"Show status code"},{"location":"basics/headers/","text":"Headers Fetches and displays the headers of the specified URL. python linkscraper -u https://example.com -a get-headers When you run the above command, it will fetch and display the headers of the URL https://example.com . Filter headers Upon running the command, the tool will visit the webpage at https://example.com, scrape the links found on the page, and retrieve the headers associated with those links. The results will then be filtered to only display links that match the filter criteria specified by header. python linkscraper -u https://example.com -a get-headers -filter header","title":"Get headers"},{"location":"basics/headers/#headers","text":"Fetches and displays the headers of the specified URL. python linkscraper -u https://example.com -a get-headers When you run the above command, it will fetch and display the headers of the URL https://example.com .","title":"Headers"},{"location":"basics/headers/#filter-headers","text":"Upon running the command, the tool will visit the webpage at https://example.com, scrape the links found on the page, and retrieve the headers associated with those links. The results will then be filtered to only display links that match the filter criteria specified by header. python linkscraper -u https://example.com -a get-headers -filter header","title":"Filter headers"},{"location":"basics/static/css/","text":"CSS The -get-css-files command is designed to extract and list all Cascading Style Sheets (CSS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website. To fetch all CSS files from the website https://example.com , you can execute the following command: python linkscraper -u https://example.com -a get-css-files Filter The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-css-files -filter example.css Show minify files The -smf option filters the listed files to display only minified ones identified as .min.css . python linkscraper -u https://example.com -a get-css-files -smf Download You can also download all the listed files easily; simply use the -d flag. python linkscraper -u https://example.com -a get-css-files -d","title":"CSS"},{"location":"basics/static/css/#css","text":"The -get-css-files command is designed to extract and list all Cascading Style Sheets (CSS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website. To fetch all CSS files from the website https://example.com , you can execute the following command: python linkscraper -u https://example.com -a get-css-files","title":"CSS"},{"location":"basics/static/css/#filter","text":"The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-css-files -filter example.css","title":"Filter"},{"location":"basics/static/css/#show-minify-files","text":"The -smf option filters the listed files to display only minified ones identified as .min.css . python linkscraper -u https://example.com -a get-css-files -smf","title":"Show minify files"},{"location":"basics/static/css/#download","text":"You can also download all the listed files easily; simply use the -d flag. python linkscraper -u https://example.com -a get-css-files -d","title":"Download"},{"location":"basics/static/images/","text":"Images The -get-images-files command is designed to extract and list all JavaScript (JS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website. To fetch all JS files from the website https://example.com , you can execute the following command: python linkscraper -u https://example.com -a get-images-files Filter The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-images-files -filter example.png Download You can also download all the listed files easily; simply use the -d flag. python linkscraper -u https://example.com -a get-images-files -d Formats of images Linkscraper is compatible with the main image formats used on the modern internet and also supports some formats that aren't widely used today, aiming to enhance the command's compatibility and to ensure no image format is left out. PNG SVG TIFF WebP AVIF JPEG JPEG XR JPEG 2000","title":"Images"},{"location":"basics/static/images/#images","text":"The -get-images-files command is designed to extract and list all JavaScript (JS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website. To fetch all JS files from the website https://example.com , you can execute the following command: python linkscraper -u https://example.com -a get-images-files","title":"Images"},{"location":"basics/static/images/#filter","text":"The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-images-files -filter example.png","title":"Filter"},{"location":"basics/static/images/#download","text":"You can also download all the listed files easily; simply use the -d flag. python linkscraper -u https://example.com -a get-images-files -d","title":"Download"},{"location":"basics/static/images/#formats-of-images","text":"Linkscraper is compatible with the main image formats used on the modern internet and also supports some formats that aren't widely used today, aiming to enhance the command's compatibility and to ensure no image format is left out. PNG SVG TIFF WebP AVIF JPEG JPEG XR JPEG 2000","title":"Formats of images"},{"location":"basics/static/js/","text":"JavaScript The -get-js-files command is designed to extract and list all JavaScript (JS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website. To fetch all JS files from the website https://example.com , you can execute the following command: python linkscraper -u https://example.com -a get-js-files Filter The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-js-files -filter example.css Show minify files The -smf option filters the listed files to display only minified ones identified as .min.css . python linkscraper -u https://example.com -a get-js-files -smf Download You can also download all the listed files easily; simply use the -d flag. python linkscraper -u https://example.com -a get-js-files -d","title":"JavaScript"},{"location":"basics/static/js/#javascript","text":"The -get-js-files command is designed to extract and list all JavaScript (JS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website. To fetch all JS files from the website https://example.com , you can execute the following command: python linkscraper -u https://example.com -a get-js-files","title":"JavaScript"},{"location":"basics/static/js/#filter","text":"The -filter option allows users to refine their results by including only those entries that match the filter keyword. python linkscraper -u https://example.com -a get-js-files -filter example.css","title":"Filter"},{"location":"basics/static/js/#show-minify-files","text":"The -smf option filters the listed files to display only minified ones identified as .min.css . python linkscraper -u https://example.com -a get-js-files -smf","title":"Show minify files"},{"location":"basics/static/js/#download","text":"You can also download all the listed files easily; simply use the -d flag. python linkscraper -u https://example.com -a get-js-files -d","title":"Download"},{"location":"overview/dependencies/","text":"Dependencies The Linkscraper requires the following libraries to function: beautifulsoup4 cloudscraper pyfiglet pyperclip requests selenium whois rich python-dotenv","title":"Dependencies"},{"location":"overview/dependencies/#dependencies","text":"The Linkscraper requires the following libraries to function: beautifulsoup4 cloudscraper pyfiglet pyperclip requests selenium whois rich python-dotenv","title":"Dependencies"},{"location":"overview/external-apis/","text":"External API's use The Linkscraper utilizes the following APIs to provide additional features through the tool's plugins: Imgur VirusTotal IP-API who.is threatcrowd Google Fonts","title":"External API's use"},{"location":"overview/external-apis/#external-apis-use","text":"The Linkscraper utilizes the following APIs to provide additional features through the tool's plugins: Imgur VirusTotal IP-API who.is threatcrowd Google Fonts","title":"External API's use"},{"location":"overview/parameters/","text":"Parameters Core Parameter Description Required Default -u, --url URL to scan \u2705 in live mode -a, --action Run an action No get-core -p, --plugin Load a plugin No -oel, --only-external-links Show only external links No Null -ssc, --show-status-code Show status code No Null -smf, --show-minify-files Show only minify files No Null -f, --filter Filter data No -d, --download Download static files No Null -we, --write-env Write environments file (.env) No Null -v, --version Show current version No Null The parameters -oel , -ssc , -smf , -we , and -d cannot take values. Plugins These parameters are only useful when used with some plugin. Parameter Description Required Default -b, --browser Set the browser to take screenshot No firefox -t, --title Set title the screenshot on Imgur No Screenshot made by Linkscraper -gf, --google-fonts Download fonts from Google Fonts No Null -up, --upload Upload the screenshot to Imgur No Null The parameters -up and -gf cannot take values.","title":"Parameters"},{"location":"overview/parameters/#parameters","text":"","title":"Parameters"},{"location":"overview/parameters/#core","text":"Parameter Description Required Default -u, --url URL to scan \u2705 in live mode -a, --action Run an action No get-core -p, --plugin Load a plugin No -oel, --only-external-links Show only external links No Null -ssc, --show-status-code Show status code No Null -smf, --show-minify-files Show only minify files No Null -f, --filter Filter data No -d, --download Download static files No Null -we, --write-env Write environments file (.env) No Null -v, --version Show current version No Null The parameters -oel , -ssc , -smf , -we , and -d cannot take values.","title":"Core"},{"location":"overview/parameters/#plugins","text":"These parameters are only useful when used with some plugin. Parameter Description Required Default -b, --browser Set the browser to take screenshot No firefox -t, --title Set title the screenshot on Imgur No Screenshot made by Linkscraper -gf, --google-fonts Download fonts from Google Fonts No Null -up, --upload Upload the screenshot to Imgur No Null The parameters -up and -gf cannot take values.","title":"Plugins"},{"location":"plugins/detect-fonts/","text":"Detect-fonts To detect fonts in the provided URL, you need to use the detect-fonts plugin. To utilize this plugin, simply enter the following command into your terminal: python linkscraper -u https://example.com -a get-plugins -p detect-fonts Google Fonts To collect variations of a specific font, simply use the -gf flag, type the font name, and press Enter. Doing this, Linkscraper will list all font files indexed by Google Fonts. python linkscraper -u https://example.com -a get-plugins -p detect-fonts -gf However, remember that to utilize Google Fonts services within Linkscraper, you'll need an Google Fonts API key, which can be obtained for free. Click here to learn how. Download To download all variants of the font, you just need to add the -d flag. python linkscraper -u https://example.com -a get-plugins -p detect-fonts -gf -d","title":"detect-fonts"},{"location":"plugins/detect-fonts/#detect-fonts","text":"To detect fonts in the provided URL, you need to use the detect-fonts plugin. To utilize this plugin, simply enter the following command into your terminal: python linkscraper -u https://example.com -a get-plugins -p detect-fonts","title":"Detect-fonts"},{"location":"plugins/detect-fonts/#google-fonts","text":"To collect variations of a specific font, simply use the -gf flag, type the font name, and press Enter. Doing this, Linkscraper will list all font files indexed by Google Fonts. python linkscraper -u https://example.com -a get-plugins -p detect-fonts -gf However, remember that to utilize Google Fonts services within Linkscraper, you'll need an Google Fonts API key, which can be obtained for free. Click here to learn how.","title":"Google Fonts"},{"location":"plugins/detect-fonts/#download","text":"To download all variants of the font, you just need to add the -d flag. python linkscraper -u https://example.com -a get-plugins -p detect-fonts -gf -d","title":"Download"},{"location":"plugins/extract-colors/","text":"Extract-colors To extract the colors used in the creation of the page from the URL provided by the user, you can utilize the extract-colors plugin. Its usage is straightforward; simply enter the following command into your terminal: python linkscraper -u https://example.com -a get-plugins -p extract-colors This plugin has a limitation: it can only recognize colors that fit into four specific patterns. See some examples below: #fff #ffffff rgb(255, 255, 255) rgba(255, 255, 255, 1)","title":"extract-colors"},{"location":"plugins/extract-colors/#extract-colors","text":"To extract the colors used in the creation of the page from the URL provided by the user, you can utilize the extract-colors plugin. Its usage is straightforward; simply enter the following command into your terminal: python linkscraper -u https://example.com -a get-plugins -p extract-colors This plugin has a limitation: it can only recognize colors that fit into four specific patterns. See some examples below: #fff #ffffff rgb(255, 255, 255) rgba(255, 255, 255, 1)","title":"Extract-colors"},{"location":"plugins/ip-location/","text":"IP-location With the ip-location plugin, you can gather geographic information about the IP of the specified URL. To use this command, simply type it into your terminal: python linkscraper -u https://example.com -a get-plugins -p ip-location The plugin utilizes the api from ip-api.com . Its free version doesn't require any API key.","title":"ip-location"},{"location":"plugins/ip-location/#ip-location","text":"With the ip-location plugin, you can gather geographic information about the IP of the specified URL. To use this command, simply type it into your terminal: python linkscraper -u https://example.com -a get-plugins -p ip-location The plugin utilizes the api from ip-api.com . Its free version doesn't require any API key.","title":"IP-location"},{"location":"plugins/page-details/","text":"Page-details The page-details plugin extracts all metadata from the given URL, excluding CSS, JS files, and fonts, since they necessitate additional linkscraper resources. python linkscraper -u https://example.com -a get-plugins -p page-details With the plugin, you can gather the following metadata: Title Description Robots directives Viewport Charset WordPress WordPress version OG metadata ( read documentation )","title":"page-details"},{"location":"plugins/page-details/#page-details","text":"The page-details plugin extracts all metadata from the given URL, excluding CSS, JS files, and fonts, since they necessitate additional linkscraper resources. python linkscraper -u https://example.com -a get-plugins -p page-details With the plugin, you can gather the following metadata: Title Description Robots directives Viewport Charset WordPress WordPress version OG metadata ( read documentation )","title":"Page-details"},{"location":"plugins/robots/","text":"Robots The robots.txt file is a standard used by websites to communicate with web crawlers and other web robots. The file indicates which areas of the site should not be processed or scanned. These rules are set by the site administrator to ensure certain pages or directories aren't crawled and to specify a delay for crawling, among other functions. To view the robots.txt file of a domain, it's straightforward. Simply use the robots plugin by executing the following command in your terminal: python linkscraper -u https://example.com -a get-plugins -p robots","title":"robots"},{"location":"plugins/robots/#robots","text":"The robots.txt file is a standard used by websites to communicate with web crawlers and other web robots. The file indicates which areas of the site should not be processed or scanned. These rules are set by the site administrator to ensure certain pages or directories aren't crawled and to specify a delay for crawling, among other functions. To view the robots.txt file of a domain, it's straightforward. Simply use the robots plugin by executing the following command in your terminal: python linkscraper -u https://example.com -a get-plugins -p robots","title":"Robots"},{"location":"plugins/screenshot/","text":"Screenshot With the screenshot plugin, you can capture screenshots of a URL using the Selenium library. To utilize this plugin, enter the following command in your terminal: python linkscraper -u https://example.com -a get-plugins -p screenshot -b firefox The -b flag specifies which browser you'll use for the screenshot capture (ensure the software is installed on your machine). You have two browser options: Google Chrome and Mozilla Firefox. However, th # Ignore the path site/ere's a crucial caveat when using Google Chrome with this feature, as noted below. Versions starting from 114 of the Google Chrome browser are incompatible with this feature; we suggest using the Mozilla Firefox browser. Upload to Imgur To upload the screenshot to Imgur, simply use the -up flag. However, remember that to utilize Imgur services within Linkscraper, you'll need an Imgur API key, which can be obtained for free. Click here to learn how. python linkscraper -u https://example.com -a get-plugins -p screenshot -b firefox -up -t \"Title of post here\" The -t flag sets the title for the post. By default, its value is ' Screenshot made by Linkscraper '.","title":"screenshot"},{"location":"plugins/screenshot/#screenshot","text":"With the screenshot plugin, you can capture screenshots of a URL using the Selenium library. To utilize this plugin, enter the following command in your terminal: python linkscraper -u https://example.com -a get-plugins -p screenshot -b firefox The -b flag specifies which browser you'll use for the screenshot capture (ensure the software is installed on your machine). You have two browser options: Google Chrome and Mozilla Firefox. However, th # Ignore the path site/ere's a crucial caveat when using Google Chrome with this feature, as noted below. Versions starting from 114 of the Google Chrome browser are incompatible with this feature; we suggest using the Mozilla Firefox browser.","title":"Screenshot"},{"location":"plugins/screenshot/#upload-to-imgur","text":"To upload the screenshot to Imgur, simply use the -up flag. However, remember that to utilize Imgur services within Linkscraper, you'll need an Imgur API key, which can be obtained for free. Click here to learn how. python linkscraper -u https://example.com -a get-plugins -p screenshot -b firefox -up -t \"Title of post here\" The -t flag sets the title for the post. By default, its value is ' Screenshot made by Linkscraper '.","title":"Upload to Imgur"},{"location":"plugins/subdomains/","text":"Subdomains With the subdomains plugin, you can list all subdomains of the given URL. To use this command, simply type it into your terminal: python linkscraper -u https://example.com -a get-plugins -p subdomains The plugin utilizes the api from threatcrowd . Its free version doesn't require any API key.","title":"subdomains"},{"location":"plugins/subdomains/#subdomains","text":"With the subdomains plugin, you can list all subdomains of the given URL. To use this command, simply type it into your terminal: python linkscraper -u https://example.com -a get-plugins -p subdomains The plugin utilizes the api from threatcrowd . Its free version doesn't require any API key.","title":"Subdomains"},{"location":"plugins/virustotal/","text":"VirusTotal VirusTotal is a free online service that analyzes files and URLs to detect viruses, worms, trojans, and other kinds of malicious content. It uses multiple antivirus engines and website scanners to provide a comprehensive report on the potential threats associated with the uploaded content. To scan a url using the VirusTotal tool, simply enter the following command: python linkscraper -u https://example.com -a get-plugins -p virustotal However, remember that to utilize VirusTotal services within Linkscraper, you'll need an VirusTotal API key, which can be obtained for free. Click here to learn how.","title":"virustotal"},{"location":"plugins/virustotal/#virustotal","text":"VirusTotal is a free online service that analyzes files and URLs to detect viruses, worms, trojans, and other kinds of malicious content. It uses multiple antivirus engines and website scanners to provide a comprehensive report on the potential threats associated with the uploaded content. To scan a url using the VirusTotal tool, simply enter the following command: python linkscraper -u https://example.com -a get-plugins -p virustotal However, remember that to utilize VirusTotal services within Linkscraper, you'll need an VirusTotal API key, which can be obtained for free. Click here to learn how.","title":"VirusTotal"},{"location":"plugins/whois/","text":"Whois WHOIS is a query and response protocol that is used for querying databases that store an Internet resource's registered users or assignees. These resources include domain names, IP address blocks and autonomous systems, but it is also used for a wider range of other information. The whois plugin allows you to extract details about a particular domain, including ownership information, unless it's set to private. To utilize this plugin, execute the command below: python linkscraper -u https://example.com -a get-plugins -p whois This plugin provides the subsequent domain-related details: Domain name Domain registrar WHOIS server Domain creation date Expiration date","title":"whois"},{"location":"plugins/whois/#whois","text":"WHOIS is a query and response protocol that is used for querying databases that store an Internet resource's registered users or assignees. These resources include domain names, IP address blocks and autonomous systems, but it is also used for a wider range of other information. The whois plugin allows you to extract details about a particular domain, including ownership information, unless it's set to private. To utilize this plugin, execute the command below: python linkscraper -u https://example.com -a get-plugins -p whois This plugin provides the subsequent domain-related details: Domain name Domain registrar WHOIS server Domain creation date Expiration date","title":"Whois"},{"location":"plugins/apis/google-fonts/","text":"Google Fonts Integration Obtaining an API Key for Google Fonts Developer API: Google Cloud Console : Go to the Google Cloud Console . If you haven\u2019t already, sign in with your Google account. Create a new project or select an existing one. Enable the API : In the navigation menu, select \"APIs & Services\" > \"Library\". In the search bar, type \"Google Fonts Developer API\". Click on the Google Fonts Developer API from the search results. Click on the \"Enable\" button. Create Credentials : After enabling the API, you\u2019ll be directed to the API page. If not, go back to the dashboard of your project. Click on \"APIs & Services\" > \"Credentials\". Click the \"Create Credentials\" dropdown button and select \"API key\". Your new API key will be displayed. Copy this key for your use. (Optional) Restrict the API Key : For added security, you might want to restrict the API key so that it can only be used by certain IPs, apps, or other specific conditions. On the \"Credentials\" page, click on the API key you've just created. Under \"Key restriction\", choose the option that suits your needs, and follow the on-screen instructions to set the restrictions. Usage : With the API key in hand, you can use it to make calls to the Google Fonts Developer API following the official documentation. Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.","title":"Google Fonts"},{"location":"plugins/apis/google-fonts/#google-fonts-integration","text":"Obtaining an API Key for Google Fonts Developer API: Google Cloud Console : Go to the Google Cloud Console . If you haven\u2019t already, sign in with your Google account. Create a new project or select an existing one. Enable the API : In the navigation menu, select \"APIs & Services\" > \"Library\". In the search bar, type \"Google Fonts Developer API\". Click on the Google Fonts Developer API from the search results. Click on the \"Enable\" button. Create Credentials : After enabling the API, you\u2019ll be directed to the API page. If not, go back to the dashboard of your project. Click on \"APIs & Services\" > \"Credentials\". Click the \"Create Credentials\" dropdown button and select \"API key\". Your new API key will be displayed. Copy this key for your use. (Optional) Restrict the API Key : For added security, you might want to restrict the API key so that it can only be used by certain IPs, apps, or other specific conditions. On the \"Credentials\" page, click on the API key you've just created. Under \"Key restriction\", choose the option that suits your needs, and follow the on-screen instructions to set the restrictions. Usage : With the API key in hand, you can use it to make calls to the Google Fonts Developer API following the official documentation. Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.","title":"Google Fonts Integration"},{"location":"plugins/apis/imgur/","text":"Imgur API Integration To integrate with Imgur's API, follow this guide: Sign Up for an Account : If you're not an Imgur user, register here . Application Registration : Go to Imgur's developer portal and register your app. This grants API access and provides your Client ID and Secret. Form Details : Application Name : Assign a distinctive name to your app. Authorization Type : Select your preferred authorization flow. Typically, \"OAuth 2 without a callback URL\" suffices. Contact & Overview : Provide your email and a concise app description. Finalize Registration : Submit the form. Secure Your Credentials : Post-registration, you'll receive a Client ID and Client Secret . Store these securely, as they're essential for API interactions. API Calls : With your Client ID , you're set to interact with the Imgur API. Always respect Imgur's API practices and terms. Reminder : The free-tier has rate limits. For extensive API usage, consider premium plans or optimize your app to stay within free limits. Always refer to Imgur's official documentation for current details. Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.","title":"Imgur"},{"location":"plugins/apis/imgur/#imgur-api-integration","text":"To integrate with Imgur's API, follow this guide: Sign Up for an Account : If you're not an Imgur user, register here . Application Registration : Go to Imgur's developer portal and register your app. This grants API access and provides your Client ID and Secret. Form Details : Application Name : Assign a distinctive name to your app. Authorization Type : Select your preferred authorization flow. Typically, \"OAuth 2 without a callback URL\" suffices. Contact & Overview : Provide your email and a concise app description. Finalize Registration : Submit the form. Secure Your Credentials : Post-registration, you'll receive a Client ID and Client Secret . Store these securely, as they're essential for API interactions. API Calls : With your Client ID , you're set to interact with the Imgur API. Always respect Imgur's API practices and terms. Reminder : The free-tier has rate limits. For extensive API usage, consider premium plans or optimize your app to stay within free limits. Always refer to Imgur's official documentation for current details. Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.","title":"Imgur API Integration"},{"location":"plugins/apis/virustotal/","text":"VirusTotal Integration Obtaining the VirusTotal API Key: Sign Up/Log In : Go to the VirusTotal website . If you don't already have an account, you'll need to sign up. If you do, simply log in. Navigate to the API Section : Once logged in, head to your profile or account section. Look for an option related to API or API Key. The exact navigation might vary as websites update their UI, but generally, it's within the user's profile or settings area. Generate or View API Key : Depending on the platform's design, you might either directly see your API key or have an option to generate one. If provided an option to generate, click on it. Note and Store Safely : Once you have your API key, make sure to note it down and store it safely. API keys should be treated like passwords, as they provide access to the services and their capabilities. Usage Limits : VirusTotal provides a free tier with limited requests per day. If you need more requests or additional features, you might need to look into their paid or premium offerings. Implementation : When implementing the API in your application or tool, ensure that the key is stored securely and is not exposed, especially if you're distributing the application to others. Remember : The above steps provide a general guide, and the exact steps might slightly vary based on changes made to the VirusTotal website or its services. Always refer to official documentation or user guides provided by the platform for the most accurate and up-to-date information. Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.","title":"VirusTotal"},{"location":"plugins/apis/virustotal/#virustotal-integration","text":"Obtaining the VirusTotal API Key: Sign Up/Log In : Go to the VirusTotal website . If you don't already have an account, you'll need to sign up. If you do, simply log in. Navigate to the API Section : Once logged in, head to your profile or account section. Look for an option related to API or API Key. The exact navigation might vary as websites update their UI, but generally, it's within the user's profile or settings area. Generate or View API Key : Depending on the platform's design, you might either directly see your API key or have an option to generate one. If provided an option to generate, click on it. Note and Store Safely : Once you have your API key, make sure to note it down and store it safely. API keys should be treated like passwords, as they provide access to the services and their capabilities. Usage Limits : VirusTotal provides a free tier with limited requests per day. If you need more requests or additional features, you might need to look into their paid or premium offerings. Implementation : When implementing the API in your application or tool, ensure that the key is stored securely and is not exposed, especially if you're distributing the application to others. Remember : The above steps provide a general guide, and the exact steps might slightly vary based on changes made to the VirusTotal website or its services. Always refer to official documentation or user guides provided by the platform for the most accurate and up-to-date information. Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.","title":"VirusTotal Integration"},{"location":"settings/env/","text":"Environments The .env file is a plain-text configuration file where environment variables are stored. Typically used in the context of software development, these files allow developers to set environment-specific settings without changing application code. In essence, the .env file serves as a convenient and secure method for managing application configurations. To save your API keys, it's straightforward. Just enter the following command into your terminal: python linkscraper -we If you prefer not to use the automatic .env file-writing tool, you can manually create a .env file in the root directory of Linkscraper with the following content:","title":"Environments"},{"location":"settings/env/#environments","text":"The .env file is a plain-text configuration file where environment variables are stored. Typically used in the context of software development, these files allow developers to set environment-specific settings without changing application code. In essence, the .env file serves as a convenient and secure method for managing application configurations. To save your API keys, it's straightforward. Just enter the following command into your terminal: python linkscraper -we If you prefer not to use the automatic .env file-writing tool, you can manually create a .env file in the root directory of Linkscraper with the following content:","title":"Environments"}]}