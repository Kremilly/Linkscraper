{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Dive deep into the web's intricate layers with Linkscraper! Whether you're a researcher, developer, or a curious explorer, our tool efficiently scans web pages to fetch links, images, emails, and much more. Powered by an array of versatile plugins and a user-friendly interface, Linkscraper streamlines the process of extracting and managing web data. From headers to JavaScript files, and from cookies to CSS \u2013 uncover the digital signatures of the web with ease. Join us on this journey and uncover the treasures hidden in plain sight on the web.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.10 (Download)</li> <li>PIP</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Clone this repository.</p> <pre><code>git clone https://github.com/kremilly/linkscraper.git\n</code></pre> <p>To install dependencies.</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"basics/cookies/","title":"Get cookies","text":""},{"location":"basics/cookies/#cookies","title":"Cookies","text":"<p>Retrieves all cookies from the site.</p> <pre><code>python linkscraper -u https://example.com -a get-cookies\n</code></pre> <p>This will return a list of all the cookies used by <code>https://example.com</code>.</p>"},{"location":"basics/cookies/#filter","title":"Filter","text":"<p>The <code>-filter</code> option allows users to refine their results by including only those entries that match the filter keyword.</p> <pre><code>python linkscraper -u https://example.com -a get-cookies -filter cookie\n</code></pre> <p>To collect cookies from the website \"<code>https://example.com</code>\" and then filter the results to only show those related to the term \"<code>cookie</code>\"</p>"},{"location":"basics/core/","title":"Core data","text":""},{"location":"basics/core/#core","title":"Core","text":"<p>The core function is the first function that can be executed by the application, it provides the main functions concerning the connection to a certain URL that was provided by you.</p> <p>You can execute the function by running the following command:</p> <pre><code>python linkscraper -u https://example.com -a get-core\n</code></pre> <p>When you run the above command, it will fetch and request headers of the URL <code>https://example.com</code>.</p>"},{"location":"basics/get-emails/","title":"Get emails","text":""},{"location":"basics/get-emails/#get-emails","title":"Get Emails","text":"<p>By using various parameters, users can define specific actions, like collecting email addresses from the web pages. In the context of the provided command:</p> <pre><code>python linkscraper -u https://example.com -a get-emails\n</code></pre> <p>The flag is directed to scrape the website \"<code>https://example.com</code>\" with the specific action (<code>-a</code>) of retrieving email addresses (<code>get-emails</code>). This allows users to gather emails present on the given site, which can be useful for various purposes, including research, auditing, or data collection. As always, such a tool should be used ethically and with proper permissions to avoid any legal or ethical violations.</p>"},{"location":"basics/get-emails/#filter","title":"Filter","text":"<p>The <code>-filter</code> option allows users to refine their results by including only those entries that match the filter keyword.</p> <pre><code>python linkscraper -u https://example.com -a get-emails -filter example@domain.com\n</code></pre> <p>To collect emails from the website \"<code>https://example.com</code>\" and then filter the results to only show those related to the email \"<code>example@doamin.com</code>\"</p>"},{"location":"basics/get-links/","title":"Get links","text":""},{"location":"basics/get-links/#get-links","title":"Get Links","text":"<p>The <code>-get-links</code> command is designed to extract all links from a user-specified URL. Along with its primary function, it supports three additional sub-commands that we will detail below.</p> <p>The command allows for tailored actions through different parameters. A common use case is to harvest links from web pages, as demonstrated:</p> <pre><code>python linkscraper -u https://example.com -a get-links\n</code></pre>"},{"location":"basics/get-links/#filter","title":"Filter","text":"<p>The <code>-filter</code> option allows users to refine their results by including only those entries that match the filter keyword.</p> <pre><code>python linkscraper -u https://example.com -a get-links -filter domain.com\n</code></pre>"},{"location":"basics/get-links/#only-external-links","title":"Only external links","text":"<p>The <code>-oel</code> option allows users to refine their results by including only those entries that match the links outside from <code>-u</code>.</p> <pre><code>python linkscraper -u https://example.com -a get-emails -oel\n</code></pre>"},{"location":"basics/get-links/#show-status-code","title":"Show status code","text":"<p>The <code>-ssc</code> option show the status code of all links listed</p> <pre><code>python linkscraper -u https://example.com -a get-emails -ssc\n</code></pre>"},{"location":"basics/headers/","title":"Get headers","text":""},{"location":"basics/headers/#headers","title":"Headers","text":"<p>Fetches and displays the headers of the specified URL.</p> <pre><code>python linkscraper -u https://example.com -a get-headers\n</code></pre> <p>When you run the above command, it will fetch and display the headers of the URL <code>https://example.com</code>.</p>"},{"location":"basics/headers/#filter-headers","title":"Filter headers","text":"<p>Upon running the command, the tool will visit the webpage at https://example.com, scrape the links found on the page, and retrieve the headers associated with those links. The results will then be filtered to only display links that match the filter criteria specified by header.</p> <pre><code>python linkscraper -u https://example.com -a get-headers -filter header\n</code></pre>"},{"location":"basics/static/css/","title":"CSS","text":""},{"location":"basics/static/css/#css","title":"CSS","text":"<p>The <code>-get-css-files</code> command is designed to extract and list all Cascading Style Sheets (CSS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website.</p> <p>To fetch all CSS files from the website <code>https://example.com</code>, you can execute the following command:</p> <pre><code>python linkscraper -u https://example.com -a get-css-files\n</code></pre>"},{"location":"basics/static/css/#filter","title":"Filter","text":"<p>The <code>-filter</code> option allows users to refine their results by including only those entries that match the filter keyword.</p> <pre><code>python linkscraper -u https://example.com -a get-css-files -filter example.css\n</code></pre>"},{"location":"basics/static/css/#show-minify-files","title":"Show minify files","text":"<p>The <code>-smf</code> option filters the listed files to display only minified ones identified as <code>.min.css</code>.</p> <pre><code>python linkscraper -u https://example.com -a get-css-files -smf\n</code></pre>"},{"location":"basics/static/css/#download","title":"Download","text":"<p>You can also download all the listed files easily; simply use the <code>-d</code> flag.</p> <pre><code>python linkscraper -u https://example.com -a get-css-files -d\n</code></pre>"},{"location":"basics/static/images/","title":"Images","text":""},{"location":"basics/static/images/#images","title":"Images","text":"<p>The <code>-get-images-files</code> command is designed to extract and list all JavaScript (JS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website.</p> <p>To fetch all JS files from the website <code>https://example.com</code>, you can execute the following command:</p> <pre><code>python linkscraper -u https://example.com -a get-images-files\n</code></pre>"},{"location":"basics/static/images/#filter","title":"Filter","text":"<p>The <code>-filter</code> option allows users to refine their results by including only those entries that match the filter keyword.</p> <pre><code>python linkscraper -u https://example.com -a get-images-files -filter example.png\n</code></pre>"},{"location":"basics/static/images/#download","title":"Download","text":"<p>You can also download all the listed files easily; simply use the <code>-d</code> flag.</p> <pre><code>python linkscraper -u https://example.com -a get-images-files -d\n</code></pre>"},{"location":"basics/static/images/#formats-of-images","title":"Formats of images","text":"<p>Linkscraper is compatible with the main image formats used on the modern internet and also supports some formats that aren't widely used today, aiming to enhance the command's compatibility and to ensure no image format is left out.</p> <ul> <li>PNG</li> <li>SVG</li> <li>TIFF</li> <li>WebP</li> <li>AVIF</li> <li>JPEG</li> <li>JPEG XR</li> <li>JPEG 2000</li> </ul>"},{"location":"basics/static/js/","title":"JavaScript","text":""},{"location":"basics/static/js/#javascript","title":"JavaScript","text":"<p>The <code>-get-js-files</code> command is designed to extract and list all JavaScript (JS) files from a user-specified URL. This is particularly useful for web developers, designers, and security professionals who wish to review or analyze the style assets of a website.</p> <p>To fetch all JS files from the website <code>https://example.com</code>, you can execute the following command:</p> <pre><code>python linkscraper -u https://example.com -a get-js-files\n</code></pre>"},{"location":"basics/static/js/#filter","title":"Filter","text":"<p>The <code>-filter</code> option allows users to refine their results by including only those entries that match the filter keyword.</p> <pre><code>python linkscraper -u https://example.com -a get-js-files -filter example.css\n</code></pre>"},{"location":"basics/static/js/#show-minify-files","title":"Show minify files","text":"<p>The <code>-smf</code> option filters the listed files to display only minified ones identified as <code>.min.css</code>.</p> <pre><code>python linkscraper -u https://example.com -a get-js-files -smf\n</code></pre>"},{"location":"basics/static/js/#download","title":"Download","text":"<p>You can also download all the listed files easily; simply use the <code>-d</code> flag.</p> <pre><code>python linkscraper -u https://example.com -a get-js-files -d\n</code></pre>"},{"location":"overview/dependencies/","title":"Dependencies","text":"<p>The Linkscraper requires the following libraries to function:</p> <ul> <li>beautifulsoup4</li> <li>cloudscraper</li> <li>pyfiglet</li> <li>pyperclip</li> <li>requests</li> <li>selenium</li> <li>whois</li> <li>rich</li> <li>python-dotenv</li> </ul>"},{"location":"overview/external-apis/","title":"External API's use","text":"<p>The Linkscraper utilizes the following APIs to provide additional features through the tool's plugins:</p> <ul> <li>Imgur</li> <li>VirusTotal</li> <li>IP-API</li> <li>who.is</li> <li>threatcrowd</li> <li>Google Fonts</li> </ul>"},{"location":"overview/parameters/","title":"Parameters","text":""},{"location":"overview/parameters/#parameters","title":"Parameters","text":""},{"location":"overview/parameters/#core","title":"Core","text":"Parameter Description Required Default -u, --url URL to scan \u2705 in live mode -a, --action Run an action No <code>get-core</code> -p, --plugin Load a plugin No -oel, --only-external-links Show only external links No <code>Null</code> -ssc, --show-status-code Show status code No <code>Null</code> -smf, --show-minify-files Show only minify files No <code>Null</code> -f, --filter Filter data No -d, --download Download static files No <code>Null</code> -we, --write-env Write environments file (.env) No <code>Null</code> -v, --version Show current version No <code>Null</code> <p>The parameters <code>-oel</code>, <code>-ssc</code>, <code>-smf</code>, <code>-we</code>, and <code>-d</code> cannot take values.</p>"},{"location":"overview/parameters/#plugins","title":"Plugins","text":"<p>These parameters are only useful when used with some plugin.</p> Parameter Description Required Default -b, --browser Set the browser to take screenshot No <code>firefox</code> -t, --title Set title the screenshot on Imgur No <code>Screenshot made by Linkscraper</code> -gf, --google-fonts Download fonts from Google Fonts No <code>Null</code> -up, --upload Upload the screenshot to\u00a0Imgur No <code>Null</code> <p>The parameters <code>-up</code> and <code>-gf</code> cannot take values.</p>"},{"location":"plugins/detect-fonts/","title":"detect-fonts","text":""},{"location":"plugins/detect-fonts/#detect-fonts","title":"Detect-fonts","text":"<p>To detect fonts in the provided URL, you need to use the <code>detect-fonts</code> plugin. To utilize this plugin, simply enter the following command into your terminal:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p detect-fonts\n</code></pre>"},{"location":"plugins/detect-fonts/#google-fonts","title":"Google Fonts","text":"<p>To collect variations of a specific font, simply use the <code>-gf</code> flag, type the font name, and press Enter. Doing this, Linkscraper will list all font files indexed by Google Fonts.</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p detect-fonts -gf\n</code></pre> <p>However, remember that to utilize Google Fonts services within Linkscraper, you'll need an Google Fonts API key, which can be obtained for free. Click here to learn how.</p>"},{"location":"plugins/detect-fonts/#download","title":"Download","text":"<p>To download all variants of the font, you just need to add the <code>-d</code> flag.</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p detect-fonts -gf -d\n</code></pre>"},{"location":"plugins/extract-colors/","title":"extract-colors","text":""},{"location":"plugins/extract-colors/#extract-colors","title":"Extract-colors","text":"<p>To extract the colors used in the creation of the page from the URL provided by the user, you can utilize the <code>extract-colors</code> plugin. Its usage is straightforward; simply enter the following command into your terminal:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p extract-colors\n</code></pre> <p>This plugin has a limitation: it can only recognize colors that fit into four specific patterns. See some examples below:</p> <ul> <li><code>#fff</code></li> <li><code>#ffffff</code></li> <li><code>rgb(255, 255, 255)</code></li> <li><code>rgba(255, 255, 255, 1)</code></li> </ul>"},{"location":"plugins/ip-location/","title":"ip-location","text":""},{"location":"plugins/ip-location/#ip-location","title":"IP-location","text":"<p>With the ip-location plugin, you can gather geographic information about the IP of the specified URL. To use this command, simply type it into your terminal:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p ip-location\n</code></pre> <p>The plugin utilizes the api from ip-api.com. Its free version doesn't require any API key.</p>"},{"location":"plugins/page-details/","title":"page-details","text":""},{"location":"plugins/page-details/#page-details","title":"Page-details","text":"<p>The <code>page-details</code> plugin extracts all metadata from the given URL, excluding CSS, JS files, and fonts, since they necessitate additional <code>linkscraper</code> resources.</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p page-details\n</code></pre> <p>With the plugin, you can gather the following metadata:</p> <ul> <li>Title</li> <li>Description</li> <li>Robots directives</li> <li>Viewport</li> <li>Charset</li> <li>WordPress</li> <li>WordPress version</li> <li>OG metadata (read documentation)</li> </ul>"},{"location":"plugins/robots/","title":"robots","text":""},{"location":"plugins/robots/#robots","title":"Robots","text":"<p>The <code>robots.txt</code> file is a standard used by websites to communicate with web crawlers and other web robots. The file indicates which areas of the site should not be processed or scanned. These rules are set by the site administrator to ensure certain pages or directories aren't crawled and to specify a delay for crawling, among other functions.</p> <p>To view the <code>robots.txt</code> file of a domain, it's straightforward. Simply use the <code>robots</code> plugin by executing the following command in your terminal:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p robots\n</code></pre>"},{"location":"plugins/screenshot/","title":"screenshot","text":""},{"location":"plugins/screenshot/#screenshot","title":"Screenshot","text":"<p>With the <code>screenshot</code> plugin, you can capture screenshots of a URL using the Selenium library. To utilize this plugin, enter the following command in your terminal:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p screenshot -b firefox\n</code></pre> <p>The <code>-b</code> flag specifies which browser you'll use for the screenshot capture (ensure the software is installed on your machine). You have two browser options: Google Chrome and Mozilla Firefox. However, th</p> <p># Ignore the path site/ere's a crucial caveat when using Google Chrome with this feature, as noted below.</p> <p>Versions starting from 114 of the Google Chrome browser are incompatible with this feature; we suggest using the Mozilla Firefox browser.</p>"},{"location":"plugins/screenshot/#upload-to-imgur","title":"Upload to Imgur","text":"<p>To upload the screenshot to Imgur, simply use the <code>-up</code> flag. However, remember that to utilize Imgur services within Linkscraper, you'll need an Imgur API key, which can be obtained for free. Click here to learn how.</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p screenshot -b firefox -up -t \"Title of post here\"\n</code></pre> <p>The <code>-t</code> flag sets the title for the post. By default, its value is '<code>Screenshot made by Linkscraper</code>'.</p>"},{"location":"plugins/subdomains/","title":"subdomains","text":""},{"location":"plugins/subdomains/#subdomains","title":"Subdomains","text":"<p>With the subdomains plugin, you can list all subdomains of the given URL. To use this command, simply type it into your terminal:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p subdomains\n</code></pre> <p>The plugin utilizes the api from threatcrowd. Its free version doesn't require any API key.</p>"},{"location":"plugins/virustotal/","title":"virustotal","text":""},{"location":"plugins/virustotal/#virustotal","title":"VirusTotal","text":"<p>VirusTotal is a free online service that analyzes files and URLs to detect viruses, worms, trojans, and other kinds of malicious content. It uses multiple antivirus engines and website scanners to provide a comprehensive report on the potential threats associated with the uploaded content.</p> <p>To scan a url using the VirusTotal tool, simply enter the following command:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p virustotal\n</code></pre> <p>However, remember that to utilize VirusTotal services within Linkscraper, you'll need an VirusTotal API key, which can be obtained for free. Click here to learn how.</p>"},{"location":"plugins/whois/","title":"whois","text":""},{"location":"plugins/whois/#whois","title":"Whois","text":"<p>WHOIS is a query and response protocol that is used for querying databases that store an Internet resource's registered users or assignees. These resources include domain names, IP address blocks and autonomous systems, but it is also used for a wider range of other information.</p> <p>The <code>whois</code> plugin allows you to extract details about a particular domain, including ownership information, unless it's set to private. To utilize this plugin, execute the command below:</p> <pre><code>python linkscraper -u https://example.com -a get-plugins -p whois\n</code></pre> <p>This plugin provides the subsequent domain-related details:</p> <ul> <li>Domain name</li> <li>Domain registrar</li> <li>WHOIS server</li> <li>Domain creation date</li> <li>Expiration date</li> </ul>"},{"location":"plugins/apis/google-fonts/","title":"Google Fonts","text":""},{"location":"plugins/apis/google-fonts/#google-fonts-integration","title":"Google Fonts Integration","text":"<p>Obtaining an API Key for Google Fonts Developer API:</p> <ol> <li> <p>Google Cloud Console:</p> </li> <li> <p>Go to the Google Cloud Console.</p> </li> <li>If you haven\u2019t already, sign in with your Google account.</li> <li>Create a new project or select an existing one.</li> <li> <p>Enable the API:</p> </li> <li> <p>In the navigation menu, select \"APIs &amp; Services\" &gt; \"Library\".</p> </li> <li>In the search bar, type \"Google Fonts Developer API\".</li> <li>Click on the Google Fonts Developer API from the search results.</li> <li>Click on the \"Enable\" button.</li> <li> <p>Create Credentials:</p> </li> <li> <p>After enabling the API, you\u2019ll be directed to the API page. If not, go back to the dashboard of your project.</p> </li> <li>Click on \"APIs &amp; Services\" &gt; \"Credentials\".</li> <li>Click the \"Create Credentials\" dropdown button and select \"API key\".</li> <li>Your new API key will be displayed. Copy this key for your use.</li> <li> <p>(Optional) Restrict the API Key:</p> </li> <li> <p>For added security, you might want to restrict the API key so that it can only be used by certain IPs, apps, or other specific conditions.</p> </li> <li>On the \"Credentials\" page, click on the API key you've just created.</li> <li>Under \"Key restriction\", choose the option that suits your needs, and follow the on-screen instructions to set the restrictions.</li> <li> <p>Usage:</p> </li> <li> <p>With the API key in hand, you can use it to make calls to the Google Fonts Developer API following the official documentation.</p> </li> </ol> <p>Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.</p>"},{"location":"plugins/apis/imgur/","title":"Imgur","text":""},{"location":"plugins/apis/imgur/#imgur-api-integration","title":"Imgur API Integration","text":"<p>To integrate with Imgur's API, follow this guide:</p> <ol> <li>Sign Up for an Account: If you're not an Imgur user, register here.</li> <li>Application Registration: Go to Imgur's developer portal and register your app. This grants API access and provides your Client ID and Secret.</li> <li>Form Details:</li> <li>Application Name: Assign a distinctive name to your app.</li> <li>Authorization Type: Select your preferred authorization flow. Typically, \"OAuth 2 without a callback URL\" suffices.</li> <li>Contact &amp; Overview: Provide your email and a concise app description.</li> <li>Finalize Registration: Submit the form.</li> <li>Secure Your Credentials: Post-registration, you'll receive a <code>Client ID</code> and <code>Client Secret</code>. Store these securely, as they're essential for API interactions.</li> <li>API Calls: With your <code>Client ID</code>, you're set to interact with the Imgur API. Always respect Imgur's API practices and terms.</li> </ol> <p>Reminder: The free-tier has rate limits. For extensive API usage, consider premium plans or optimize your app to stay within free limits. Always refer to Imgur's official documentation for current details.</p> <p>Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.</p>"},{"location":"plugins/apis/virustotal/","title":"VirusTotal","text":""},{"location":"plugins/apis/virustotal/#virustotal-integration","title":"VirusTotal Integration","text":"<p>Obtaining the VirusTotal API Key:</p> <ol> <li> <p>Sign Up/Log In :</p> </li> <li> <p>Go to the VirusTotal website.</p> </li> <li> <p>If you don't already have an account, you'll need to sign up. If you do, simply log in.</p> </li> <li> <p>Navigate to the API Section :</p> </li> <li> <p>Once logged in, head to your profile or account section.</p> </li> <li> <p>Look for an option related to API or API Key. The exact navigation might vary as websites update their UI, but generally, it's within the user's profile or settings area.</p> </li> <li> <p>Generate or View API Key :</p> </li> <li> <p>Depending on the platform's design, you might either directly see your API key or have an option to generate one. If provided an option to generate, click on it.</p> </li> <li> <p>Note and Store Safely :</p> </li> <li> <p>Once you have your API key, make sure to note it down and store it safely. API keys should be treated like passwords, as they provide access to the services and their capabilities.</p> </li> <li> <p>Usage Limits :</p> </li> <li> <p>VirusTotal provides a free tier with limited requests per day. If you need more requests or additional features, you might need to look into their paid or premium offerings.</p> </li> <li> <p>Implementation :</p> </li> <li> <p>When implementing the API in your application or tool, ensure that the key is stored securely and is not exposed, especially if you're distributing the application to others.</p> </li> </ol> <p>Remember : The above steps provide a general guide, and the exact steps might slightly vary based on changes made to the VirusTotal website or its services. Always refer to official documentation or user guides provided by the platform for the most accurate and up-to-date information.</p> <p>Visit the Environments page to learn how to save the key in your .env file so that Linkscraper starts using your key.</p>"},{"location":"settings/env/","title":"Environments","text":""},{"location":"settings/env/#environments","title":"Environments","text":"<p>The <code>.env</code> file is a plain-text configuration file where environment variables are stored. Typically used in the context of software development, these files allow developers to set environment-specific settings without changing application code.</p> <p>In essence, the <code>.env</code> file serves as a convenient and secure method for managing application configurations.</p> <p>To save your API keys, it's straightforward. Just enter the following command into your terminal:</p> <pre><code>python linkscraper -we\n</code></pre> <p>If you prefer not to use the automatic .env file-writing tool, you can manually create a .env file in the root directory of Linkscraper with the following content:</p>"}]}